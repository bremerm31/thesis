\chapter{Conclusion}

While load balancing and local timestepping certainly display great promise for the acceleration of hurricane storm surge, their implementation in a storm surge code is left as future work. In this chapter we conclude with a careful analysis of the impact of this work on hurricane storm surge, examine local timestepping through a Tao of parallelism analysis~\cite{Pingali2011}, and conclude with thoughts on storm surge simulations in post-Moore's era computing.

\section{Implications for hurricane storm surge}
Here we attempt to give insight on the impact of these performance optimizations on real time forecasting of hurricane storm surge.

\subsection{Scalabitity of Devastator}
Here we reprise the strong scaling analysis performed on HPX with Devastator. %Using the adaptive local timestepping algorithm, we scale out the lake at rest problem with 500

%%\subsection{Compute work estimate for any hurricane}
%%Try to come up with impact on time to solution

\section{Tao analysis of adaptive timestepping}
The tao of parallelism categorizes algorithms based on salient features to guide parallel implementations~\cite{Pingali2011}. Programs are thought of as a graph of abstract data types (ADT) and algorithms. In the case of DG finite element methods, the abstract data types correspond to the submeshes, and the edges of the graph are used to reason about dependencies, i.e. which flux buffers need to have been processed before proceeding. The algorithm corresponds to the timestepping scheme, which exchanges flux buffers and updates elements.
The tao analysis then examines three main features shown in Figure~\ref{fig:tao-analysis}.

\begin{itemize}
\item {\bf T}opology: The topology refers to the ADT graph, for example, if a task is embarassingly parallel, this might be represented as a graph with an empty edge set. Graphs are classified by the amount of information required to describe them, ranging from structured to semi-structured to unstructured. For our work, we consider the submesh graph generated by the partitionining of a dual graph of a planar mesh. The most important invariant of the graph is that it remains static throughout the simulation. Since we are able to utilize this information to guarantee once an event can be safely processed, we refer to the graph as semi-structured.
\item {\bf A}ctive nodes: At any given point in the simulation, active nodes are nodes which may perform  compute. In the case of the task-based synchronous timestepping, a submesh may only update once it has received the update boundary state from each of the submesh's neighbors, i.e. once all of its dependencies to do so have been satisfied. The topic of active nodes is split into two further subcategories, {\em location} and {\em ordering}. Location emphases whether the active nodes are purely determined by the graph (referred to as topology-driven) or if the state also influences whether or not the algorithm is able to advance (referred to as data-drive). For both synchronous and adaptive local timestepping algorithms, the methods are data-driven. Based on whether or not a submesh has processed a message determines whether or not it can advance to the next timestep, i.e. is it active or not. The second subcategory is {\em ordering}. Timestepping methods are inherently ordered. It is non-sensical to have a submesh update in a non-sequential order.
\item {\bf O}perator: Operators are applied to the ADTs. If the operator changes the topology of the graph it is classified as {\em morph}. A {\em local computation} updates the state of the ADT, and a {\em reader} changes neither the state nor the topology. All of our timestepping methods are local computations. Updating the state has no impact on which submeshes to wait on for the subsequent update.
\end{itemize}
All three timestepping schemes for hurricane storms surge---asynchronous timestepping with a fixed timestep, asynchronous local timestepping, and synchronous timestepping---rely one the same underlying ADT, with an irregular static topology, consist of operators that perform local computation, and have data-driven ordered active nodes. To distinguish between the types of parallelism between each of the timestepping scheme, we rely on the further classification of ordered algorithms by Hassaan~\cite{Hassaan1, Hassaan2}. Hassaan uses the notion of {\em Kinetic Dependence Graph} (KDG) to classify ordered algorithms. The KDE is defined by a triple $(G,P,U)$, where:
\begin{itemize}
\item $G$ is the task graph,
\item $P$ is a {\em safe source test},
\item $U$ is an update rule.
\end{itemize}
The task graph looks similar to the graph associated with the ADTs, except that in this cases directed edges correspond to the next pending flux buffer exchange. The update rule corresponds to the method for updating the ADT, i.e. stepping the mesh forward in time. The key distinction between the three timestepping methods comes from the safe source test. Given an active node of the graph, i.e. a submesh that can be updated, this function examines the graph, and determines whether or not that submesh can locally update. If the safe source test is trivial and any active node can update in parallel, the method is called {\em source stable}.

Coming now back to the timestepping schemes, the asynchronous fixed timestepping algorithm is an example of a source stable KDE, once all messages have been processed at a timestamp, we can safely---here in reference to the algorithm, not the numerical stability---advance to the next timestep. This importantly is contrasted with the other two methods in which case a non-local condition (the CFL condition) needs to be satisfied in order to ensure that the submesh can be safely updated, i.e. how can I be certain that the timestep currently proposed will not need to be shortened. Note that this strictly arises due to the fact the shallow water equations are non-linear. Mathematically, the safe-source test requires examining the state of the full domain of dependence. For linear systems of equations, this problem is statically determined, and so timesteps can be proposed (even local timesteps) such that the source test can be guaranteed to be satisfied by construction.

For the synchronous adaptive timestep, the safe source test simply checks all cells, ensuring that the domain of dependence is inside the checked region. On the other hand, for the adaptive local timestepping algorithm, we assume that general the source test will evaluate to true and then rollback in the case that that assumption was incorrect. The performance optimizations outline in Section~\ref{sec:performance-optimization} then provide two important limitations to bad speculation. The {\em Reducing unnecessary speculation} fix can be thought of as tighter policing of which nodes are active. This fix acknowledges outstanding push fluxes from neighbors as dependencies, and so the node forgoes the executing of further updates until these waited upon messages arrive, and the node effectively reactivates. The {\em Avoiding small timesteps due to binning} can be thought of as the introduction of a non-trivial source test. By examining the state of an active node, we are able to defer execution until a message from the neighbor arrives. However, rather than wait for the safe source test to be resolved, we introduce dependencies by forcing selected neighbors to synchronize. 
Areas of future work include coming up with better source stable tests. In particular, considering the lake at rest problem on the uniform mesh, even though the set of equations is non-linear the solution is source stable by construction, and insofar it should be possible to eliminate bad speculation. This could be thought of as being equivalent to enforcing the dependencies of a locally linearlized local timestepping problems, and then only speculating when non-linearities dominate the evolution. A second optimization that may be fruitful is leverage the static nature of the ADT graph topology. Whereas in the case of simulating billiard balls using discrete event simulation, any billiard ball may collide with any other billiard ball, for the simulation of conservation laws, these collisions, which can be thought of as deviations from the linearized task graph, can only occur to a set of static neighbors. This information may be fruitfully leveraged by devastator to create more efficient algorithms for bounding GVT.

Amorphous data-parallelism arises out of tao-analysis to quantify optimal parallel performance. {\em Amorphous data parallelism} describes the amount of parallelism found in active nodes throughout the simulation. Comparing parallelism profiles allows informs what performance benefits we may expect to see. Comparing the fixed asynchronous timestepping scheme with the synchronous adaptive timestepping method we see little difference in available parallelism. The difference being the cost of the synchronization associated with updating the global timestep. The direct impact may be unclear since the taking a global adaptive timestep may reduce the overall work. As an example, fixed timestep size for year long simulations of the Australian coast are limited by a brief period during the rainy season where inland estuaries flood. Navigating this trade-off is a problem specific measure. With the deciding factors requiring navigating the cost of the all-reduces versus the suboptimal timestepping for portions of the simulation. For the locally adaptive method, we can expect to see any critical path shortening also found for the adaptive synchronous timestepping algorithm. We may experience slight shortening of the critical path through good speculation, but we have not found a use case where this plays a dominant role. Primarily, the benefit of the local timestepping can be attributed to work reduction. The amount of parallelism decrease since there are fewer active nodes. By enabling speculation, the aim is to keep more nodes active allowing more work to be done. The synchronizations, which in this case are managed through GVT are hidden behind useful work, and furthermore, distributed performance studies done for traffic simulations suggest that Devastator should be able to hide the cost of GVT over core counts relevant for storm surge simulation~\cite{Chan2018}. Comparing the adaptive local timestepping scheme to a synchronous adaptive timestepping scheme we see that our scheme has an approximately similar amount of amorphous parallelism. However, the difference between the two is that we in our case do not appear to pay significantly for the cost of the synchronizations. Since it has been found in practice to be too expensive to synchronize globally for an adaptive timestep, we expect that the ability to hide the cost of synchronization in this case to deliver similar performance benefits. One notion that undercuts this argument is that the amount of work has been fundamentally reduced. In that sense, it may be that the cost of synchronization decreases to acceptable levels. Nevertheless, the adaptive local timestepping method's amorphous parallelism profile suggests that the algorithm primarily improves upon existing timestepping methods through work reduction, and may have an impact on time to solution through critical path reduction.


\section{Impact of the end of Moore's Law}

We conclude this thesis with a look towards the future. Moore's law, the techno-economic factors which have led to the exponential growth in computer performance over the past 50 years is ending as physical limitations in lithography are attained~\cite{Shalf2020}. Rather than producing faster general purpose hardware, solutions at the computer architecutre level will have to use transistors more efficiently to provide continued perfomance gains in computing. These architectures will become increasingly specialized for specific problems, resulting in computing systems with a swiss army knife of accelerators~\cite{EH2018}. These changes are already underway within the machine learning community with start-ups such as Tenstorrent, Cerebaras, and SambaNova as well as initiatives under industry giants such as Microsoft's Project Catapult, Facebook's Big Sur nodes, and Google's tensor processing units. While the economic drivers for industry primarily revolve around accelerating machine learning workflows, these innovations will translate to scientific computing with mixed success.

The technical difficulty associated with effectively using these new computers threatens to split the scientific computing community into two camps: those whose applications are able to efficiently run on these new architectures, and those who see limited benefit from new architectures either due to hardware constraints and/or the high opportunity cost of extensive code refactoring. Originally, the road map to exascale had proposed a two path solution, one based on manycore processors and the other based on GPUs. The difficulties in excising performance from the second generation Intel Xeon Phi (KNL) architecture and subsequently weak demand for the product, lead to the shuttering of CPU path as Intel cancelled the Xeon Phi product line. The Exascale Computing Initiative has since fully committed to the GPU path towards an exascale machine. With two exascale GPU-based clusters---Oak Ridge Leadership Computing Facility (OLCF)'s Frontier and Argonne Leadership Computing Facility's Aurora---scheduled for delivery in 2021. The interests of the winners and losers of these post-CPU architectures are apparent in the contrasting recent acquisitions made by OLCF and the Texas Advanced Computing Center (TACC). The most recent acquisition by TACC was Frontera, a predominantly Intel Xeon cluster. These server class x86 nodes are substantially less power efficient, at a peak performance ($R_{\max}$) of 23.5 PFLOPS at roughly 6 MW of power consumption (3.9 PFLOPS/MW), compared to OLCF's GPU-based Summit, which was acquired one year earlier and boasts a peak performance ($R_{\max}$) of 149 PFLOPS at 10 MW of of consumption (14.9 PFLOPS/MW). The trade-off is that the Intel Xeon Frontera-based architecture requires very little modification to existing code bases, greatly improving immediate usability. Whereas tremendous effort has been poured into making applications performant on GPUs with substantial funding from the US Department of Energy (DOE). This divergence in hardware and application readiness will lead to a schism in scientific computing where access to the most performant machines is restricted to a few highly optimized applications, while the remainder of the scientific computing community will continue to use machines like Frontera to achieve their research goals. Through improvements in programming models and code portability, these post-CPU architectures may slowly achieve more widespread adoption.

At the same time, the field of computational science is not in stasis. Different numerical methods will become more commonly used, reflecting funding priorities as well as competitve advantages of software frameworks. Regardless of the adoption of post-Moore architectures, the fundamental design constraints are incontrovertible. Algorithms which can effectively leverage the increasing FLOPS to bandwidth ratios will ultimately win out over codes that do not. The algorithms currently used to simulate hurricane storm surge, low order stencil codes, are certainly expected to be among the losers. We propose three research directions which may lead to improved performance of storm surge simulation on future architectures:
\begin{enumerate}
\item {\em Stable high order discretizations}:
  The impact of high order methods has already been extensively studied~\cite{CEED,Warburton,Gassner,ADERDG, Pazner} and specifically examined in the context of the shallow water equations in~\cite{Brus, Kopera}. For smooth solutions, not only do we achieve higher accuracy per degree of freedom and are also able to coarsen the mesh, which decreases the number of required timesteps, the larger mass matrices associated with each element require a higher arithmetic intensity to update. For memory bound applications, this additional computational work generates no performance penalty as memory traffic remains the performance bottleneck.
  However, the emphasis on stable is not to be underestimated. In the case of the shallow water equations, specifically stable treatment of the wet-dry interface remains a thorny issue. Particularly promising approaches include the use of WENO like algorithms to accurately compute derivatives near the wet-dry interface~\cite{Kopera} and projection methods that map troubled high-order cells to more refined low-order cells~\cite{Rannabauer}. Lastly, we also recommend the adoption of the CEED library, the DOE's exascale high-order finite element discretization library. This library emphasizes performance portability onto both GPU and CPU architectures, and would firewall the storm surge application from changes introduced at the architecture level. Further exploration is required to determine whether or not this library provides the correct level of abstraction and flexibility.
\item {\em Lower floating point precision}: Another way to reduce the bandwidth requirements is to make the solution smaller. By moving from double precision to lower precision solutions, the amount of data loaded from memeory can be significantly reduced. Work by D\"{u}ben and Palmer has examined the impact of reduced precision on weather simulation~\cite{Dueben2014}. Findings suggest that the size of floating point numbers could be reduced to 15 bits with no visible degradation of the solution. Extension of these results to hurricane storm surge must be done cautiously. Careful analysis must be done to demonstrate that numerical accuracy remains higher than the uncertainty associated with the inputs (storm track, Manning's $n$, coast bathymetry). This could introduce significant speed-ups with minimally invasive modifcations to the code.
\item {\em Exploiting data parallelism of ensemble runs}: Ultimately, we do not care about the performance of a single run, but rather the overall performance of an ensemble of runs. These runs are entirely independent of one another, but execute the same code on the same mesh. Assuming that we have a natural size, e.g. a cache line width or warp size, we propose solving that many solutions concurrently thereby exposing data parallelism. Thus, we would ensure that all data in the cache line is being used, lowering the amount of data required to be loaded from memory. This would particularly help with low order methods, where the sparse matrix-vector products associated with exchanging flux information at the boundary cause significant numbers of cache misses. The impact of this approach on the time to solution remains unclear. While we certainly would induce fewer cache misses, we would have to scale out the single run which executes multiple simulations further than the multiple separate runs.  However, overall compute resource utilization would increase. For problems for which time to solution is less important and numerous runs are required, e.g. updating flood insurance maps or uncertainty quantification~\cite{Lindley}, we would expect to see significant savings in core-hours used.
\end{enumerate}


%% Need for high order methods
%% Impact of reduced precision simulation