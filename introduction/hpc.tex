\section{Exascale Computing--Novel Programming Models}

%Even as the speed of supercomputers is drastically increasing, the end of Moore's law and the introduction of many-core architectures represent a tectonic shift in the HPC community.
%In particular, the degree of hardware parallelism is increasing at an exponential rate and the cost of data movement and synchronization is increasing faster than the cost of computation~\cite{Kogge2013}.
% , and hardware is becoming increasingly irregular due to the use of accelerators and susceptibility to failure.
% In order to achieve good resource utilization on these machines, task-based programming and execution models are being developed to
%As a result, asynchronous task-based programming has been of great interest due to its potential to express increased software parallelism, introduce more flexible load balancing capabilities, and hide the cost of communication through task over-decomposition.
%Examples of major task-based programming and execution models include Charm++~\cite{charm++}, HPX~\cite{hpx2}, Legion~\cite{legion}, OCR~\cite{ocr}, PaRSEC~\cite{parsec}, StarPU~\cite{starpu}, etc.
%There are also domain-specific task-based programming systems, such as the Uintah AMR Framework~\cite{uintah}, and task-based portability layers such as DARMA~\cite{darma}.
% Additionally, task-based execution may help address issues such as hardware fault tolerance.
%These programming models decouple the specification of the algorithm from the task scheduling mechanism, which determines where and when each task may execute and orchestrates the movement of required data between the tasks.
%Furthermore, lightweight, one-sided messaging protocols that support active messages have the potential to reduce the overheads associated with inter-process communication and synchronization, which will become even more important as parallelism increases.

%\subsubsection{Active Global Address Space}

%The most relvant abstraction with respects to load balancing is the active global address space (AGAS). As we intend to implement dynamic load balancing using an HPX framework, we describe the AGAS here.
%While the threading subsystem operates within a single private address space, HPX extends its programming model to distributed runs via an \emph{active global address space}. Global address spaces attempt to emulate the ease of programming on a single node, while still maintaining tight control over data locality necessary for writing a performant distributed code. Two well-known global address space models are UPC~\cite{upc} and Co-Array Fortran~\cite{coarray}. While global address space models like UPC's partitioned global address space (PGAS) are more data-centric, e.g. by exposing pointers to memory addresses on different nodes, HPX approaches global address spaces in a more object-oriented manner.

%AGAS consists of a collection of private address spaces, called \emph{localities}. Each locality will run its own instance of the threading subsystem, scheduling threads with locally available resources. In practice, localities are typically chosen to be  nodes or NUMA domains. The basic addressable unit in AGAS is the \emph{component}. Components encapsulate the objects the user would like to remotely access. To interact with a component, the user must go through a smart-pointer-like wrapper class called a \emph{client}. The client can not only manage the component's lifetime via the RAII idiom, but also exposes remotely invokable member functions. Clients can either reside on the same or different locality as their associated component. When a remote locality executes a client member function, HPX will send an active message to the locality where the component is located, execute the function there, and return the result to the client. By interfacing with components through clients, AGAS provides equivalent local and distributed semantics, simplifying the programming of distributed applications.

%The key difference between AGAS and other global address space models is its native support for component \emph{migration}. HPX's AGAS layer allows the developer to relocate components to different localities during runtime. With all component functions being invoked through the client interface, HPX is able to ensure that the component functions invoked through the client will be executed on the correct localities, guaranteeing the application's correctness. This functionality can be used to accelerate applications via dynamic load balancing. However, since component migration potentially requires sending large quantities of data through the interconnect, and the load profile is application dependent, HPX requires the application to manage component relocation.

%AGAS decouples the correctness of the application from the data distribution across nodes.  This abstraction provides a means of ensuring load balance across nodes without imposing unreasonable burdens on the application to guarantee correctness.

%\subsubsection{Dynamic Load Balancing}
%One of the key aspects of DGSWEM is its ability to simulate coastal inundation during hurricane landfall.
%The DG kernel can be implemented in a manner such that dry regions of the simulation require no computational work; however, as the hurricane inundates the coast, this optimization introduces significant dynamic load imbalance.
%In order to address the load imbalance while still fitting the problem in machine memory, two constraints (load and memory) must be accounted for simultaneously.

%While multi-constraint graph partitioning tools have been used to obtain good static partitions for these scenarios, they perform sub-optimally for irregular applications such as ours.

%In adaptive mesh refinement (AMR) codes and $hp$-adaptive finite elements, block-structured grids are commonly used, which allow for efficient representations of data locality using space filling curves. These space filling curves allow for dynamic load balancing techniques, known as geometric partitioning and outlined in \cite{Devine2005,Burstedde2011,Blaise2012,Ferreira2017}. However, for our purposes block-structured grids have difficulties modeling complex coastal geometries. As such, DGSWEM utilizes unstructured grids. Dynamic load balancing for these grids typically relies on two approaches: (1) graph partitioning algorithms, such as those provided in the METIS and SCOTCH libraries \cite{scotch,Bhatele2012,Karypis1998,Devine2005,Aykanat2007}, and (2) diffusion or refinement based approaches \cite{Schloegel1997}.
%The simulation of coastal inundation introduces irregularity, which decouples the memory and load balancing constraints.
%, i.e. balancing elements across processors is not sufficient
%To the authors' knowledge, the only paper that uses dynamic load balancing to address this issue is \cite{Asuncion2016}. However, their approach only balances load on structured grids, which may result in memory overflow.
%Local timestepping methods introduce similar irregularity.
%Seny et al. have proposed a static load balancing scheme using multi-constraint partitioning in \cite{Seny2014}.
%However, they note the dynamic load balancing problem as an open one.
%Some examples of load balancing algorithm evaluations in the context of task-based execution models include the use of cellular automata \cite{Hosoori2011}, hierarchical partitioning \cite{Zheng2010}, and gossip protocols \cite{Menon2013}.


Task-based programming models have arisen in response to increased hardware concurrency and irregularity. To identify what makes multithreaded code slow, the Ste{\textbar}{\textbar}ar group---who develop HPX---use the acronym SLOW:
\begin{itemize}
\item {\bf S}tarvation: cores idling due to insufficient parallelism exposed by the application,
\item {\bf L}atency: delays induced by waiting on dependencies, e.g. waiting on messages which are sent through a cluster's interconnect,
\item {\bf O}verhead: additional work performed for a multithreaded application which is unnecessary in a sequential implementation,
\item {\bf W}aiting for contention resolution: delays associated with the accessing of shared resources between threads.
\end{itemize}
These factors are prevalent in any multi-threaded code including those that use a task-based runtime. However task-based runtimes aim to outperform their more traditional counterparts by mitigating the impacts of SLOW, e.g. by hiding message latencies or aggressive work stealing to address starvation.

Even as there appears to be no convergence to a single task-based runtime in the HPC community, there appears to be broad agreement on the abstractions being used. Most runtimes favor some task-graph or dataflow based abstraction for algorithm design. Once the task graph has been specified it is given to the runtime to be executed. Another commonly used abstraction is that of globally addressable objects. By making actors or objects globally addressable, the application is able to specify dependencies between objects with similar syntax for both shared and distributed memory parallelizations.

While simply porting algorithms will certainly allow us to mitigate the impact of hardware irregularity, we have found that in practice this gives small to moderate improvement over existing optimized MPI implementations.
In~\cite{Bremer2019}, we found that HPX---one such runtime---gives a speed-up of 1.2 over an MPI implementation on 256 Knights Landing nodes. Results suggest that the speed-up was mainly attributed to page faults for the MPI implementation, rather than avoiding message latencies or MPI overhead. In fact, we suspect that this gap could largely be closed through the use of a better memory allocator for the MPI implementation.

This limited improvement can be attributed to two main factors. Firstly, Moore's law has not yet ended, and we have not yet truly entered the age of extreme heterogeneity where managing the SLOW factors will become increasingly important.
Furthermore and specifically related to storm surge, given the relatively modest computational resource requirement of a real-time forecast, the dependence on legacy codes, and the low arithmetic intensity of the storm surge codes (both ADCIRC and low-order DG solvers), there still remains a hesitation to adopt non-CPU based architectures that may provide more FLOPs but whose impact on time to solution is unclear.
%\todo{Talk more here about expected future architectures}

Secondly, the test case used in~\cite{Bremer2019} ultimately lacks irregularity. The communication profile of the code reduces to a stencil code on an unstructured mesh, which can be implemented efficiently using non-blocking point to point MPI messages. What is needed to generate a value proposal for using task-based runtime systems are sources of irregularity. Algorithms need to be more adaptive and perform compute only where needed. While doing so makes the implementation certainly more difficult, this complexity is passed off to the runtime.

Then, the aim of this work is to identify or introduce sources of adaptivity into the storm surge code, which are are then efficiently mapped onto a runtime. This thesis is split into two main chapters describing the impact of two such ideas:
\begin{enumerate}
\item {\em Chapter \ref{ch:lb}}: A key feature of storm surge simulation is the classification of cells as either wet or dry. The relevant computational impact of this classification is that while wet cells require the evaluation of the full physics to update, dry cells trivially update. As the storm makes landfall, inland cells wet and create a load imbalance. This chapter explores the impact of dynamic load balancing strategies, whereby we move cells across processor ranks during the simulation to ensure that each rank must update an equal number of cells.
\item {\em Chapter \ref{ch:lts}}: The timestep taken by these simulations is restricted by the Courant-Friedrichs-Lewy (CFL) condition. The CFL condition forms the basis of the stability results which have led to the popularity of finite volume and DG methods. However, this condition is ultimately a condition that can be enforced locally, and in light of significant variations in mesh size and advection speeds, local CFL enforcement can lead to dramatic reduction in work. This chapter develops a timestepping method that locally enforces the CFL condition and recovers the same stability results for the global timestepping case. The proposed timestepping scheme is then parallelized using an optimistic (speculative) parallel discrete event simulator.
\end{enumerate}