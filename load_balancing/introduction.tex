\section{Introduction}
% why hurricane simulations are important
%Hurricane events are incredibly deadly and costly natural disasters.
Since 1980, seven out of the ten most costly US climate disasters were hurricanes, with Hurricane Katrina being the most expensive\cite{noaa}. Hurricanes Harvey, Maria, and Irma, which occurred in 2017, are expected to be among the five most costly US natural disasters.
The utilization of computational models can provide local officials with high fidelity tools to assist in evacuation efforts and mitigate loss of life and property.
%why we need HPC
Due to the highly nonlinear nature of hurricane dynamics and stringent time constraints, high performance computing (HPC) plays a cornerstone role in providing accurate predictions of flooding.
Because of the importance of fast, efficient models, there is a significant interest in improving the speed and quality of these computational tools.

%how computing is changing
Even as the speed of supercomputers is drastically increasing, the end of Moore's law and the introduction of many-core architectures represent a tectonic shift in the HPC community.
In particular, the degree of hardware parallelism is increasing at an exponential rate and the cost of data movement and synchronization is increasing faster than the cost of computation~\cite{KoggeCiSE2014}.
% , and hardware is becoming increasingly irregular due to the use of accelerators and susceptibility to failure.
% In order to achieve good resource utilization on these machines, task-based programming and execution models are being developed to
As a result, asynchronous task-based programming has been of great interest due to its potential to express increased software parallelism, introduce more flexible load balancing capabilities, and hide the cost of communication through task over-decomposition ~\cite{charm,hpx,legion,ocr,parsec,starpu,uintah,darma}.
%Examples of major task-based programming and execution models include Charm++, HPX~\cite{hpx}, Legion~\cite{legion}, OCR~\cite{ocr}, PaRSEC~\cite{parsec}, StarPU~\cite{starpu}, etc.
%There are also domain-specific task-based programming systems, such as the Uintah AMR Framework~\cite{uintah}, and task-based portability layers such as DARMA~\cite{darma}.
% Additionally, task-based execution may help address issues such as hardware fault tolerance.
These programming models decouple the specification of the algorithm from the task scheduling mechanism, which determines where and when each task may execute and orchestrates the movement of required data between the tasks.
Furthermore, lightweight, one-sided messaging protocols that support active messages have the potential to reduce the overheads associated with inter-process communication and synchronization, which will become even more important as parallelism increases.
However, transitioning scientific applications from their current synchronous implementations to asynchronous, task-based programming models requires a significant software engineering effort.
Additionally, load balancing the execution of the resulting task graph remains a challenge due to the underlying computational hardness of optimal task scheduling.
% Often, load balancers must utilize application-specific information to obtain a reasonable schedule.

% DGSWEM and the load balancing problem
This paper examines the potential benefits of implementing DGSWEM, a discontinuous Galerkin (DG) finite-element storm surge code, on a task-based asynchronous execution model, and investigates various load balancing strategies for the resulting program.
One of the key aspects of DGSWEM is its ability to simulate coastal inundation during hurricane landfall.
The DG kernel can be implemented in a manner such that dry regions of the simulation require no computational work; however, as the hurricane inundates the coast, this optimization introduces significant dynamic load imbalance.
In order to address the load imbalance while still fitting the problem in machine memory, two constraints (load and memory) must be accounted for simultaneously.

While multi-constraint graph partitioning tools have been used to obtain good static partitions for these scenarios, they perform sub-optimally for irregular applications such as ours.
%On the other hand, to address the dynamic nature of the simulation, dynamic load balancing strategies have been studied extensively.
%However, these approaches typically are based on single constraint partitioning strategies.
On the other hand, fully dynamic load balancing strategies are typically based on balancing a single constraint, which is insufficient for hurricane simulation.
In order to overcome these shortcomings, we investigate dynamic and semi-static multi-constraint load balancing approaches that can be leveraged to run hurricane simulations efficiently on future supercomputing platforms.

% our approach
To circumvent the software engineering effort to port DGSWEM to a task-based model and to allow extremely rapid evaluation of the resulting execution under a variety of parameters (e.g. choice of load balancing algorithm), we have developed DGSim, a task-based execution model and discrete-event simulation tool that features asynchronously migratable, globally addressable objects.
DGSim was designed to natively model lightweight, one-sided, active interprocess messaging along with distributed, asynchronously migratable objects.  Together, these allow the simulation to fully overlap both the computation of the new tile placements and the resulting movement of the tile data with the application's main computation.  These features are not readily available in other simulation tools but are key features of the fully asynchronous load balancing algorithms introduced here.
The development of DGSim allows us to estimate the performance of hurricane simulations running with these advanced features in a variety of configurations.
% DGSim is additionally able to mimic mechanisms that are common across production frameworks, such as automated task scheduling, load balancing, and data movement.
Crucially, DGSim allows us to simulate the computational behavior of DGSWEM in an asynchronous task-based runtime using skeletonization, eliminating the need to execute the costly computational kernels, resulting in a lightweight approach that allows for rapid algorithm prototyping and evaluation.
The main contributions of this paper are:
\begin{enumerate}
\item The development of multi-constraint dynamic load balancing strategies specifically geared for the irregularity associated with the simulation of hurricane storm surge. In doing so, we present a dynamic and semi-static algorithm.
\item The \emph{trellis} approach for semi-static repartitiong strategies which fully leverages the asynchronous nature of the run-time. This approach can be extended to ensure efficient semi-static load balancing for a wide range of problems.
\item The development and validation of DGSim, a discrete-event simulator that enables rapid prototyping and evaluation of {\em fully asynchronous} load balancers for task-based parallel programs.
\end{enumerate}

%outline of paper
The outline of the paper is as follows. Section~\ref{sec:related} presents related work.  In Section \ref{sec:dgswem}, we discuss the DG kernel and the irregular nature of the inundation problem. Thereafter, we outline DGSim, which simulates the performance of an asynchronous task-based implementation of DGSWEM. Section \ref{sec:balancers} presents formalism for load balancing in an asynchronous context and outlines a distributed diffusion-based and a semi-static load balancing algorithm. Lastly, in Section \ref{sec:experiments}, we present our model validation and experimental results, demonstrating the viability of these load balancing approaches.
